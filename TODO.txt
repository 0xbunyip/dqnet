TODO:
- Implement transfer learning by loading part of network's weights
- Implement screen recording
- Test speed on GPU and check CPU-GPU memory roundtrip
- Add function descriptions and comments
- Move half experiences to GPU
- Test Double Q-learning

DOING:
- Add memory logging and garbage collection code
- Save experience and add continue-training feature

DONE:
- Clean up code, shorten params' names
- Read network type from network's weights file
- Move clone_target to Network class
- Separate the case of not fixing Q-target
- Clone target network right after creation
- Test Q-learning on deterministic game
- Scale pixel's value on GPU
- Fix get first screen of each episode
- Choose network type from run file or from runtime arguments
- Fix RNG bug in test games
- Add network description to info file
- Join network and target_network in 1 class
- Reduce network's weights file size by pickling only numpy arrays
- Initialize network weights using HeUniform
- Implement RMSProp
- Test loading network params
- Allow display game screen while evaluating
- Clip error term of Q-learning update to [-1, 1]
- Clip reward of each step to [-1, 1]
- Add random actions at the start of each episode
- Unify Random number generators
- Save network's weight and report loss/reward each epoch
- Add basic timing module
- Do action and terminal masking instead of fancy indexing in network
- Use None for minibatch sizes
- Join state and next_state of returned experience
- Get multiple indices for each iteration in get_random_minibatch
- Add evaluation module
- Split train and evaluation experience
- Split agent off environment
- Add epsilon to get_action (evaluation action has different epsilon)
- Place hyper-parameters into a single place
- Check number of observations have been seen at the start of each episode
- Save some frames at the start to validate average Q values on those frames
- Change evaluate_fn to return action instead of values matrix
- Test Q-learning and Network with small grid game
