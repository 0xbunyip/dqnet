TODO:
- Unify Random number generators
- Add random actions at the start of each episode
- Clip error term of Q-learning update to [-1, 1]
- Clip reward of each step to [-1, 1]
- Change RMSProp parameters
- Test speed on GPU
- Check CPU-GPU memory roundtrip
- Clean up code, shorten params' names
- Add function descriptions and comments
- Move half experiences to GPU
- Test Double Q-learning

DONE:
- Save network's weight and report loss/reward each epoch
- Add basic timing module
- Do action and terminal masking instead of fancy indexing in network
- Use 2 function with different minibatch sizes OR use None for minibatch sizes
- Join state and next_state of returned experience
- Get multiple indices for each iteration in get_random_minibatch
- Add evaluation module
- Split train and evaluation experience
- Split agent off environment
- Add epsilon to get_action (evaluation action has different epsilon)
- Place hyper-parameters into a single place
- Check number of observations have been seen at the start of each episode
- Save some frames at the start to validate average Q values on those frames
- Change evaluate_fn to return action instead of values matrix
- Test Q-learning and Network with small grid game